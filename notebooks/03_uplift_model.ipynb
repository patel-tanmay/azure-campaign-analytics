{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d52a24-7b41-429a-9983-36aac3e21f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Set the storage account name\n",
    "storage_account = \"stcampaigntp\"  # update if your storage account is different\n",
    "\n",
    "# Step 2: Mount access using secret\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "  dbutils.secrets.get(scope=\"local-scope\", key=\"storage-account-key\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c01034-6f1f-4ffa-b842-54fa4537db77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# üîÅ Create or get Spark session (if not already)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# üì• Load the feature-enhanced dataset\n",
    "df = spark.read.parquet(f\"abfss://curated@{storage_account}.dfs.core.windows.net/criteo-1m-features\")\n",
    "\n",
    "# ‚úÖ Optional: Check the schema\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac20f4ea-5191-4721-8fa1-57a8d44bf0bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Step 2: Add uplift_label (4-class) and uplift_binary_label (binary target)\n",
    "df = df.withColumn(\n",
    "    \"uplift_label\",\n",
    "    when((col(\"treatment\") == 0) & (col(\"conversion\") == 0), 0)\n",
    "    .when((col(\"treatment\") == 0) & (col(\"conversion\") == 1), 1)\n",
    "    .when((col(\"treatment\") == 1) & (col(\"conversion\") == 0), 2)\n",
    "    .when((col(\"treatment\") == 1) & (col(\"conversion\") == 1), 3)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"uplift_binary_label\",\n",
    "    when((col(\"treatment\") == 1) & (col(\"conversion\") == 1), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Confirm added columns\n",
    "df.select(\"treatment\", \"conversion\", \"uplift_label\", \"uplift_binary_label\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e4d12f7-1354-4ecc-ad17-2194ef55b30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è If your dataset is too large, consider sampling before converting to Pandas\n",
    "# Example: df = df.sample(fraction=0.2)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "pandas_df = df.toPandas()\n",
    "print(f\"‚úÖ Converted to pandas with shape: {pandas_df.shape}\")\n",
    "\n",
    "# Confirm value counts for stratification\n",
    "print(\"\\nüéØ Value counts of uplift_label:\")\n",
    "print(pandas_df['uplift_label'].value_counts())\n",
    "\n",
    "# Train/Test Split (Stratified by uplift_label)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    pandas_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=pandas_df['uplift_label']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Train shape: {train_df.shape} | Test shape: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b0d13a-2c52-4e50-a162-2ec817970e2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define columns used for modeling (all renamed features)\n",
    "feature_cols = ['I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12']\n",
    "\n",
    "# Check that all columns exist in the DataFrame\n",
    "missing = [col for col in feature_cols if col not in train_df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected feature columns: {missing}\")\n",
    "\n",
    "print(\"‚úÖ Feature columns defined:\", feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af99777c-6131-40ac-99dd-5b13515ce92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80/20 split for training and validation\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"‚úÖ Train set shape: {train_df.shape}\")\n",
    "print(f\"‚úÖ Validation set shape: {valid_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a89a1799-d825-44b4-be99-d01fbc98e61f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# üü© Create class transformation label\n",
    "# Only retain rows where class transformation is meaningful\n",
    "train_df['uplift_binary_label'] = np.where(\n",
    "    (train_df['treatment'] == 1) & (train_df['conversion'] == 1), 1,\n",
    "    np.where((train_df['treatment'] == 0) & (train_df['conversion'] == 1), 0, np.nan)\n",
    ")\n",
    "\n",
    "valid_df['uplift_binary_label'] = np.where(\n",
    "    (valid_df['treatment'] == 1) & (valid_df['conversion'] == 1), 1,\n",
    "    np.where((valid_df['treatment'] == 0) & (valid_df['conversion'] == 1), 0, np.nan)\n",
    ")\n",
    "\n",
    "# üü¶ Two-Model Approach: Split treated and control groups\n",
    "train_treated = train_df[train_df['treatment'] == 1]\n",
    "train_control = train_df[train_df['treatment'] == 0]\n",
    "\n",
    "valid_treated = valid_df[valid_df['treatment'] == 1]\n",
    "valid_control = valid_df[valid_df['treatment'] == 0]\n",
    "\n",
    "# üß™ Diagnostic printout\n",
    "print(f\"üì¶ Prepared modeling datasets:\")\n",
    "print(f\"Two-Model: Treated ‚Üí {train_treated.shape} | Control ‚Üí {train_control.shape}\")\n",
    "print(f\"Class Transformation: {train_df[['uplift_binary_label']].dropna().shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b4463e-2c9d-4b6d-88e8-a5766613f14a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional: Set random_state for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# -------------------------------\n",
    "# üü© Logistic Regression ‚Äì Two Model\n",
    "# -------------------------------\n",
    "logit_treat = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "logit_control = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "\n",
    "logit_treat.fit(train_treated[feature_cols], train_treated['conversion'])\n",
    "logit_control.fit(train_control[feature_cols], train_control['conversion'])\n",
    "\n",
    "# -------------------------------\n",
    "# üü© Logistic Regression ‚Äì Class Transformation\n",
    "# -------------------------------\n",
    "train_ct = train_df.dropna(subset=['uplift_binary_label'])\n",
    "\n",
    "logit_ct = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "logit_ct.fit(train_ct[feature_cols], train_ct['uplift_binary_label'])\n",
    "\n",
    "# -------------------------------\n",
    "# üü¶ Random Forest ‚Äì Two Model\n",
    "# -------------------------------\n",
    "rf_treat = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "rf_control = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "\n",
    "rf_treat.fit(train_treated[feature_cols], train_treated['conversion'])\n",
    "rf_control.fit(train_control[feature_cols], train_control['conversion'])\n",
    "\n",
    "# -------------------------------\n",
    "# üü¶ Random Forest ‚Äì Class Transformation\n",
    "# -------------------------------\n",
    "rf_ct = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "rf_ct.fit(train_ct[feature_cols], train_ct['uplift_binary_label'])\n",
    "\n",
    "print(\"‚úÖ All models trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6b7ed9-078b-418c-96f7-911e2cff15d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create test set (entire dataset used here for simplicity)\n",
    "X_test = train_df[feature_cols]\n",
    "T_test = train_df['treatment']\n",
    "Y_test = train_df['conversion']\n",
    "\n",
    "# -------------------------------\n",
    "# üü© Logistic Regression ‚Äì Two-Model\n",
    "# -------------------------------\n",
    "p_treat_logit = logit_treat.predict_proba(X_test)[:, 1]\n",
    "p_control_logit = logit_control.predict_proba(X_test)[:, 1]\n",
    "uplift_logit_twomodel = p_treat_logit - p_control_logit\n",
    "\n",
    "# -------------------------------\n",
    "# üü© Logistic Regression ‚Äì Class Transformation\n",
    "# -------------------------------\n",
    "uplift_logit_ct = logit_ct.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# -------------------------------\n",
    "# üü¶ Random Forest ‚Äì Two-Model\n",
    "# -------------------------------\n",
    "p_treat_rf = rf_treat.predict_proba(X_test)[:, 1]\n",
    "p_control_rf = rf_control.predict_proba(X_test)[:, 1]\n",
    "uplift_rf_twomodel = p_treat_rf - p_control_rf\n",
    "\n",
    "# -------------------------------\n",
    "# üü¶ Random Forest ‚Äì Class Transformation\n",
    "# -------------------------------\n",
    "uplift_rf_ct = rf_ct.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"‚úÖ Uplift scores predicted for all models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea94198-4b4e-449d-9117-14c5ca544212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklift.metrics import qini_curve\n",
    "from sklift.viz import plot_qini_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare dictionary of model outputs\n",
    "uplift_scores = {\n",
    "    \"LR (Two-Model)\": uplift_logit_twomodel,\n",
    "    \"LR (Class Trans.)\": uplift_logit_ct,\n",
    "    \"RF (Two-Model)\": uplift_rf_twomodel,\n",
    "    \"RF (Class Trans.)\": uplift_rf_ct\n",
    "}\n",
    "\n",
    "# Set plot style\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"üéØ Qini Curves ‚Äì Uplift Model Comparison\", fontsize=14)\n",
    "\n",
    "# Plot each model‚Äôs Qini curve\n",
    "for model_name, uplift in uplift_scores.items():\n",
    "    plot_qini_curve(Y_test.values, uplift, T_test.values, label=model_name)\n",
    "\n",
    "# Final plot touches\n",
    "plt.legend()\n",
    "plt.xlabel(\"Share of Population Targeted (%)\")\n",
    "plt.ylabel(\"Incremental Conversions (Qini)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22c9c83-56a3-4528-bed1-a09005e336d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklift.metrics import qini_auc_score\n",
    "\n",
    "# Store model names and Qini scores\n",
    "qini_scores = {}\n",
    "\n",
    "# Loop through all models and calculate Qini AUC\n",
    "for model_name, uplift in uplift_scores.items():\n",
    "    score = qini_auc_score(y_true=Y_test.values, uplift=uplift, treatment=T_test.values)\n",
    "    qini_scores[model_name] = score\n",
    "\n",
    "# Display scores\n",
    "print(\"üìä Qini Coefficients (Area Under Qini Curve):\")\n",
    "for model, score in sorted(qini_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fe30392-0443-4ef9-acc8-e93741950b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def simulate_gain_curve(y_true, treatment, uplift_scores, model_name):\n",
    "    data = pd.DataFrame({\n",
    "        'treatment': treatment,\n",
    "        'conversion': y_true,\n",
    "        'uplift_score': uplift_scores\n",
    "    })\n",
    "    \n",
    "    data = data.sort_values(by='uplift_score', ascending=False).reset_index(drop=True)\n",
    "    total_samples = len(data)\n",
    "    \n",
    "    # Simulate at 10% intervals\n",
    "    percents = np.arange(0.1, 1.1, 0.1)\n",
    "    incremental_conversions = []\n",
    "    \n",
    "    for pct in percents:\n",
    "        cutoff = int(pct * total_samples)\n",
    "        targeted = data.iloc[:cutoff]\n",
    "        \n",
    "        treated = targeted[targeted['treatment'] == 1]\n",
    "        control = targeted[targeted['treatment'] == 0]\n",
    "        \n",
    "        # Conversion rates\n",
    "        cr_treated = treated['conversion'].mean() if not treated.empty else 0\n",
    "        cr_control = control['conversion'].mean() if not control.empty else 0\n",
    "        uplift = cr_treated - cr_control\n",
    "        \n",
    "        incremental_conversions.append(uplift)\n",
    "    \n",
    "    return percents, incremental_conversions\n",
    "\n",
    "# Plot for each model\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model_name, uplift_scores_model in uplift_scores.items():\n",
    "    x, y = simulate_gain_curve(Y_test.values, T_test.values, uplift_scores_model, model_name)\n",
    "    plt.plot(x * 100, y, label=model_name)\n",
    "\n",
    "plt.title(\"üìà Simulated Incremental Lift vs % Targeted\")\n",
    "plt.xlabel(\"% of Population Targeted (Top Uplift Score)\")\n",
    "plt.ylabel(\"Incremental Conversion Lift\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f8bd0c0-4213-4a42-b24f-89d9fb5449fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Train/Test Split using Uplift Labels\n",
    "\n",
    "In uplift modeling, it‚Äôs essential that the relationship between treatment and outcome is preserved in both the training and testing data.\n",
    "\n",
    "To do this, we use stratified splitting based on the uplift_label that we previously created:\n",
    "\n",
    "0: Control group ‚Äì did not convert\n",
    "\n",
    "1: Control group ‚Äì converted\n",
    "\n",
    "2: Treated group ‚Äì did not convert\n",
    "\n",
    "3: Treated group ‚Äì converted\n",
    "\n",
    "By stratifying on this label, we maintain the same distribution of user behavior across training and test sets. This ensures our model learns and is evaluated on balanced group proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88de5bf7-e18f-42d6-9cee-9cfbd515031b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Confirm label column\n",
    "assert 'uplift_label' in pandas_df.columns, \"uplift_label column is missing.\"\n",
    "\n",
    "# Keep treatment and conversion columns as we need them later\n",
    "feature_cols = [col for col in pandas_df.columns if col.startswith('feature_')]\n",
    "X = pandas_df[feature_cols + ['treatment', 'conversion']]\n",
    "y = pandas_df['uplift_label']\n",
    "\n",
    "# Stratified Train/Test Split ‚Äî preserves uplift label distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Merge back uplift_label for analysis\n",
    "train_df = X_train.copy()\n",
    "train_df['uplift_label'] = y_train\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df['uplift_label'] = y_test\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# Optional: plot uplift label distribution\n",
    "def plot_uplift_distribution(df, title):\n",
    "    label_names = {\n",
    "        0: 'Control - No Conversion',\n",
    "        1: 'Control - Conversion',\n",
    "        2: 'Treated - No Conversion',\n",
    "        3: 'Treated - Conversion'\n",
    "    }\n",
    "\n",
    "    label_counts = df['uplift_label'].value_counts().sort_index()\n",
    "    label_counts.index = label_counts.index.map(label_names)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.bar(label_counts.index, label_counts.values)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Count\")\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.annotate(f'{height:,}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_uplift_distribution(train_df, \"Uplift Label Distribution - Training Set\")\n",
    "plot_uplift_distribution(test_df, \"Uplift Label Distribution - Testing Set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c29f507-c26d-4ac6-826e-f856b1470dcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(pandas_df.columns.tolist())\n",
    "print(pandas_df.shape)\n",
    "pandas_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d974da0e-87d9-448a-b483-779bfa33c0ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Fix: Automatically detect valid feature columns (excluding target and treatment)\n",
    "exclude_cols = ['treatment', 'conversion', 'uplift_label', 'uplift_binary_label']\n",
    "feature_cols = [col for col in pandas_df.columns if col not in exclude_cols]\n",
    "\n",
    "print(\"‚úÖ Final feature columns used:\", feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "865bd634-f982-4d01-988a-1bf388bd94f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(train_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77a5000d-8c31-44a2-8045-c8b60203d1c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In uplift modeling, we don‚Äôt just predict conversion ‚Äî we try to predict the incremental effect of treatment.\n",
    "\n",
    "There are several modeling strategies. We‚Äôll focus on two common ones (just like the Kaggle reference):\n",
    "\n",
    "## 1. Two-Model Approach\n",
    "\n",
    "We train:\n",
    "\n",
    "One model on the treatment group\n",
    "\n",
    "Another model on the control group\n",
    "\n",
    "Then we estimate uplift by:\n",
    "\n",
    "Uplift(x) = P(convert | treated, x) - P(convert | control, x)\n",
    "\n",
    "## 2. Class Transformation Approach\n",
    "\n",
    "This converts the dataset into a binary classification problem using this rule:\n",
    "\n",
    "Treatment\t  Conversion\t    Transformed Label\n",
    "1\t            1\t            1 (Positive uplift)\n",
    "0\t            0\t            1 (Positive uplift)\n",
    "1\t            0\t            0 (Negative uplift)\n",
    "0\t            1\t            0 (Negative uplift)\n",
    "\n",
    "We can then use any binary classifier (e.g., LightGBM, Logistic Regression) to predict uplift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35fc466a-c878-47f4-9beb-0a22b95eb3d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3b07f5a-9dc4-4ac8-bf90-6225fbc7bec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5: Model Preparation for Uplift Modeling\n",
    "\n",
    "Now that we have stratified training and test sets, we will structure the data for uplift modeling using two approaches:\n",
    "\n",
    "## Two-Model Strategy\n",
    "\n",
    "Train two separate models: one on treated users, another on control users.\n",
    "\n",
    "At prediction time, compute uplift as the difference between predicted probabilities.\n",
    "\n",
    "## Class Transformation Strategy\n",
    "\n",
    "Convert treatment + outcome combinations into a binary label.\n",
    "\n",
    "This allows training a single binary classifier to directly learn uplift behavior.\n",
    "\n",
    "Both are widely used in industry and research. We‚Äôll prepare the data for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cca819b-764c-4f3f-9663-78651ea6f552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First, combine back treatment and conversion for clarity\n",
    "train_df['treatment'] = train_df['treatment'].astype(int)\n",
    "train_df['conversion'] = train_df['conversion'].astype(int)\n",
    "\n",
    "test_df['treatment'] = test_df['treatment'].astype(int)\n",
    "test_df['conversion'] = test_df['conversion'].astype(int)\n",
    "\n",
    "# üìå Prepare for Two-Model Approach\n",
    "train_treated = train_df[train_df['treatment'] == 1].copy()\n",
    "train_control = train_df[train_df['treatment'] == 0].copy()\n",
    "\n",
    "# Drop uplift_label column if still present\n",
    "for df in [train_treated, train_control]:\n",
    "    if 'uplift_label' in df.columns:\n",
    "        df.drop(columns=['uplift_label'], inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "X_treated = train_treated.drop(columns=['conversion', 'treatment'])\n",
    "y_treated = train_treated['conversion']\n",
    "\n",
    "X_control = train_control.drop(columns=['conversion', 'treatment'])\n",
    "y_control = train_control['conversion']\n",
    "\n",
    "# ‚úÖ Ready for Two-Model training\n",
    "\n",
    "# üìå Prepare for Class Transformation Strategy\n",
    "def transform_classification_label(treatment, conversion):\n",
    "    return int((treatment == 1 and conversion == 1) or (treatment == 0 and conversion == 0))\n",
    "\n",
    "train_df['uplift_binary_label'] = train_df.apply(\n",
    "    lambda row: transform_classification_label(row['treatment'], row['conversion']), axis=1\n",
    ")\n",
    "\n",
    "X_class_trans = train_df.drop(columns=['conversion', 'uplift_label', 'uplift_binary_label'])\n",
    "y_class_trans = train_df['uplift_binary_label']\n",
    "\n",
    "# ‚úÖ Ready for Class Transformation training\n",
    "\n",
    "print(\"Prepared datasets:\")\n",
    "print(\"Two-Model: Treated ‚Üí\", X_treated.shape, \"| Control ‚Üí\", X_control.shape)\n",
    "print(\"Class Transformation:\", X_class_trans.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452b44f9-deac-4a12-8f86-1f91a2d756c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reuse this model, log and register it in MLflow\n",
    "import mlflow\n",
    "\n",
    "with mlflow.start_run(run_name=\"uplift-xgboost\"):\n",
    "    mlflow.log_metric(\"uplift_at_30%\", score)\n",
    "    mlflow.set_tag(\"model_type\", \"XGBTRegressor\")\n",
    "    mlflow.set_tag(\"framework\", \"causalml\" if 'causalml' in model.__module__ else \"sklift\")\n",
    "    print(\"MLflow logging done.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_uplift_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
