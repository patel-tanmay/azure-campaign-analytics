{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5590ab87-1db2-4918-b84a-06f16c149794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Grab secrets from the local scope (do NOT print them)\n",
    "client_id       = dbutils.secrets.get(\"local-scope\",\"sp-client-id\")\n",
    "tenant_id       = dbutils.secrets.get(\"local-scope\",\"sp-tenant-id\")\n",
    "client_secret   = dbutils.secrets.get(\"local-scope\",\"sp-client-secret\")\n",
    "storage_account = dbutils.secrets.get(\"local-scope\",\"storage-account-name\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe589d8-3b77-4204-ab27-1e3286779b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure OAuth (service principal) for ADLS Gen2\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\",\n",
    "               f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9d87960-13fe-42d4-9be9-ccb1199f8aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_path = f\"abfss://raw@{storage_account}.dfs.core.windows.net/dataset.csv/\"\n",
    "display(dbutils.fs.ls(raw_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8b6f94-916b-4783-8a3f-4d1761656057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adjust filename if yours differs\n",
    "src = raw_path  # or \"criteo_uplift.csv\"\n",
    "\n",
    "df_raw = (spark.read\n",
    "          .option(\"header\",\"true\")\n",
    "          .option(\"inferSchema\",\"true\")\n",
    "          .csv(src))\n",
    "\n",
    "print(\"Row count:\", df_raw.count())\n",
    "print(\"Column count:\", len(df_raw.columns))\n",
    "display(df_raw.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be982e93-f5bd-4bf9-a38e-9af680729c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols = [c.lower() for c in df_raw.columns]\n",
    "candidate_labels = [c for c in cols if c in [\"conversion\",\"response\",\"visit\",\"label\",\"click\"]]\n",
    "candidate_treat  = [c for c in cols if c in [\"treatment\",\"exposed\",\"exposure\",\"test_group\"]]\n",
    "print(\"Label candidates:\", candidate_labels)\n",
    "print(\"Treatment candidates:\", candidate_treat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e386f051-8f56-413c-a72f-3136c2df23f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LABEL_COL = \"conversion\"  # <-- change to the actual label name found above\n",
    "TREAT_COL = \"treatment\"   # <-- if present; else set to None\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df_raw.select(*df_raw.columns)  # start from raw\n",
    "if LABEL_COL not in [c.lower() for c in df.columns]:\n",
    "    raise ValueError(\"Set LABEL_COL to an existing label column name.\")\n",
    "\n",
    "# Basic EDA: class balance\n",
    "pos_rate = df.select((col(LABEL_COL).cast(\"int\") == 1).cast(\"int\").alias(\"pos\")).agg({\"pos\":\"avg\"}).collect()[0][0]\n",
    "print(f\"Positive rate (approx): {pos_rate:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_data_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
