{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5590ab87-1db2-4918-b84a-06f16c149794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Grab secrets from the local scope (do NOT print them)\n",
    "client_id       = dbutils.secrets.get(\"local-scope\",\"sp-client-id\")\n",
    "tenant_id       = dbutils.secrets.get(\"local-scope\",\"sp-tenant-id\")\n",
    "client_secret   = dbutils.secrets.get(\"local-scope\",\"sp-client-secret\")\n",
    "storage_account = dbutils.secrets.get(\"local-scope\",\"storage-account-name\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe589d8-3b77-4204-ab27-1e3286779b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure OAuth (service principal) for ADLS Gen2\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\",\n",
    "               f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9d87960-13fe-42d4-9be9-ccb1199f8aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_path = f\"abfss://raw@{storage_account}.dfs.core.windows.net/dataset.csv/\"\n",
    "display(dbutils.fs.ls(raw_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8b6f94-916b-4783-8a3f-4d1761656057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adjust filename if yours differs\n",
    "src = raw_path  # or \"criteo_uplift.csv\"\n",
    "\n",
    "df_raw = (spark.read\n",
    "          .option(\"header\",\"true\")\n",
    "          .option(\"inferSchema\",\"true\")\n",
    "          .csv(src))\n",
    "\n",
    "print(\"Row count:\", df_raw.count())\n",
    "print(\"Column count:\", len(df_raw.columns))\n",
    "display(df_raw.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be982e93-f5bd-4bf9-a38e-9af680729c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols = [c.lower() for c in df_raw.columns]\n",
    "candidate_labels = [c for c in cols if c in [\"conversion\",\"response\",\"visit\",\"label\",\"click\"]]\n",
    "candidate_treat  = [c for c in cols if c in [\"treatment\",\"exposed\",\"exposure\",\"test_group\"]]\n",
    "print(\"Label candidates:\", candidate_labels)\n",
    "print(\"Treatment candidates:\", candidate_treat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52f51419-8df4-4dd2-af31-2fed0d0b2b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df_raw.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e386f051-8f56-413c-a72f-3136c2df23f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LABEL_COL = \"conversion\"  # <-- change to the actual label name found above\n",
    "TREAT_COL = \"treatment\"   # <-- if present; else set to None\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df_raw.select(*df_raw.columns)  # start from raw\n",
    "if LABEL_COL not in [c.lower() for c in df.columns]:\n",
    "    raise ValueError(\"Set LABEL_COL to an existing label column name.\")\n",
    "\n",
    "# Basic EDA: class balance\n",
    "pos_rate = df.select((col(LABEL_COL).cast(\"int\") == 1).cast(\"int\").alias(\"pos\")).agg({\"pos\":\"avg\"}).collect()[0][0]\n",
    "print(f\"Positive rate (approx): {pos_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac21cd1-9a71-40e0-8aad-66a54ac4a465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Key columns\n",
    "LABEL_COL = \"conversion\"\n",
    "TREAT_COL = \"treatment\"\n",
    "\n",
    "# Define main DataFrame from raw\n",
    "df = df_raw.select(\"*\")  # or select only required columns if needed\n",
    "\n",
    "# Optional: quick schema check\n",
    "df.printSchema()\n",
    "df.select(LABEL_COL, TREAT_COL).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842ddbd5-14b8-4d58-80f9-2ab245866457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "TOTAL = 1_000_000\n",
    "seed = 42\n",
    "\n",
    "# Add a random key for sampling with a fixed seed\n",
    "df_rand = df.withColumn(\"_rand\", rand(seed))\n",
    "\n",
    "# Determine strata columns\n",
    "strata_cols = [LABEL_COL]\n",
    "if TREAT_COL and TREAT_COL.lower() in [c.lower() for c in df.columns]:\n",
    "    strata_cols.append(TREAT_COL)\n",
    "\n",
    "# For simplicity, take proportional sample within each stratum\n",
    "# 1) Compute counts per stratum\n",
    "df_counts = df_rand.groupBy(*strata_cols).count().collect()\n",
    "total_count = sum(r[\"count\"] for r in df_counts)\n",
    "frac_global = TOTAL / total_count\n",
    "\n",
    "# 2) Build a fractions dict for sampleBy (works for single column);\n",
    "#    For multi-col strata, weâ€™ll filter per stratum.\n",
    "if len(strata_cols) == 1:\n",
    "    label_vals = {int(r[strata_cols[0]]): min(1.0, frac_global) for r in df_counts}\n",
    "    df_sample = df_rand.sampleBy(strata_cols[0], fractions=label_vals, seed=seed)\n",
    "else:\n",
    "    # Multi-strata manual approach\n",
    "    parts = []\n",
    "    for r in df_counts:\n",
    "        cond = None\n",
    "        for c in strata_cols:\n",
    "            cond = (col(c) == r[c]) if cond is None else (cond & (col(c) == r[c]))\n",
    "        frac = min(1.0, frac_global)  # proportional\n",
    "        parts.append(df_rand.where(cond).sample(withReplacement=False, fraction=frac, seed=seed))\n",
    "    df_sample = parts[0]\n",
    "    for p in parts[1:]:\n",
    "        df_sample = df_sample.unionByName(p)\n",
    "\n",
    "print(\"Sampled rows (pre-trim):\", df_sample.count())\n",
    "\n",
    "# 3) Trim to exactly 1,000,000 deterministically\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "w = Window.orderBy(F.col(\"_rand\"))\n",
    "df_1m = (df_sample\n",
    "         .withColumn(\"_rownum\", F.row_number().over(w))\n",
    "         .where(F.col(\"_rownum\") <= TOTAL)\n",
    "         .drop(\"_rownum\",\"_rand\"))\n",
    "\n",
    "print(\"Final sample:\", df_1m.count())\n",
    "display(df_1m.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90050f8b-e950-49fa-856e-835613650620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "curated_path = f\"abfss://curated@{storage_account}.dfs.core.windows.net/criteo_uplift_1m/\"\n",
    "(df_1m\n",
    " .repartition(8)  # small number is fine for 1M rows\n",
    " .write.mode(\"overwrite\").parquet(curated_path))\n",
    "\n",
    "print(\"Wrote:\", curated_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_data_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
