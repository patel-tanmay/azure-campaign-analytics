{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d52a24-7b41-429a-9983-36aac3e21f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Set the storage account name\n",
    "storage_account = \"stcampaigntp\"  # update if your storage account is different\n",
    "\n",
    "# Step 2: Mount access using secret\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "  dbutils.secrets.get(scope=\"local-scope\", key=\"storage-account-key\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c01034-6f1f-4ffa-b842-54fa4537db77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# üîÅ Create or get Spark session (if not already)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# üì• Load the feature-enhanced dataset\n",
    "df = spark.read.parquet(f\"abfss://curated@{storage_account}.dfs.core.windows.net/criteo-1m-features\")\n",
    "\n",
    "# ‚úÖ Optional: Check the schema\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26513d3c-5107-4872-a0c4-4548808e8e58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Define Uplift Modeling Targets\n",
    "## Business Context:\n",
    "\n",
    "To build a causal inference model like uplift modeling, we can‚Äôt just use regular binary classification labels (like \"converted\" or \"not converted\"). Instead, we need to label users based on both:\n",
    "\n",
    "Whether they were treated (received the intervention, e.g., ad/coupon)\n",
    "\n",
    "Whether they converted (made a purchase, signed up, etc.)\n",
    "\n",
    "This allows us to capture how the treatment causally affects behavior ‚Äî which is the core goal of uplift modeling.\n",
    "\n",
    "We define two types of labels:\n",
    "\n",
    "uplift_label: A 4-class label used by class-modeling approaches like ClassTransformation.\n",
    "\n",
    "uplift_binary_label: A simplified binary label (1 = converted after treatment, 0 = otherwise) ‚Äî useful for standard modeling tasks or evaluation.\n",
    "\n",
    "##Technical Details:\n",
    "\n",
    "when(...).otherwise(...) is used to assign values based on combinations of the treatment and conversion columns.\n",
    "\n",
    "The mapping for the uplift_label (4-class scheme) is as follows:\n",
    "\n",
    "treatment\tconversion\tuplift_label\tMeaning\n",
    "0\t0\t0\tControl group, no conversion\n",
    "0\t1\t1\tControl group, converted\n",
    "1\t0\t2\tTreated group, no conversion\n",
    "1\t1\t3\tTreated group, converted\n",
    "\n",
    "This format allows us to train specialized models that learn not just who is likely to convert, but who is likely to convert because of the treatment ‚Äî i.e., uplift.\n",
    "\n",
    "uplift_binary_label is only 1 if a user received the treatment and converted, otherwise it‚Äôs 0. This is often used as a naive baseline or for comparison with traditional models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac20f4ea-5191-4721-8fa1-57a8d44bf0bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Step 2: Add uplift_label (4-class) and uplift_binary_label (binary target)\n",
    "df = df.withColumn(\n",
    "    \"uplift_label\",\n",
    "    when((col(\"treatment\") == 0) & (col(\"conversion\") == 0), 0)\n",
    "    .when((col(\"treatment\") == 0) & (col(\"conversion\") == 1), 1)\n",
    "    .when((col(\"treatment\") == 1) & (col(\"conversion\") == 0), 2)\n",
    "    .when((col(\"treatment\") == 1) & (col(\"conversion\") == 1), 3)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"uplift_binary_label\",\n",
    "    when((col(\"treatment\") == 1) & (col(\"conversion\") == 1), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Confirm added columns\n",
    "df.select(\"treatment\", \"conversion\", \"uplift_label\", \"uplift_binary_label\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bebf151-4a56-43fc-950b-162c2b824a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Convert to Pandas + Stratified Train-Test Split\n",
    "## Business Context:\n",
    "\n",
    "At this stage, our goal is to prepare the dataset for model training, specifically for uplift modeling using Python libraries like sklift, xgboost, etc., which operate on Pandas DataFrames ‚Äî not Spark DataFrames.\n",
    "\n",
    "We also want to ensure that the model sees representative examples of all treatment/conversion combinations, so we apply stratified splitting based on the uplift_label.\n",
    "\n",
    "This ensures the model won‚Äôt be biased due to imbalanced label distributions ‚Äî which is particularly important for uplift modeling since real-world data often has far fewer conversions than non-conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e4d12f7-1354-4ecc-ad17-2194ef55b30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è If your dataset is too large, consider sampling before converting to Pandas\n",
    "# Example: df = df.sample(fraction=0.2)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "pandas_df = df.toPandas()\n",
    "print(f\"‚úÖ Converted to pandas with shape: {pandas_df.shape}\")\n",
    "\n",
    "# Confirm value counts for stratification\n",
    "print(\"\\nüéØ Value counts of uplift_label:\")\n",
    "print(pandas_df['uplift_label'].value_counts())\n",
    "\n",
    "# Train/Test Split (Stratified by uplift_label)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    pandas_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=pandas_df['uplift_label']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Train shape: {train_df.shape} | Test shape: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c8e912b-e115-499f-b8ea-cc1fdf74c1d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Define Feature Columns for Modeling\n",
    "## Business Context:\n",
    "\n",
    "At this stage, we explicitly define which columns from the dataset will be used as model inputs. These are the engineered numeric features (originally f0‚Äìf11) that were renamed to I1‚ÄìI12 in the feature engineering step.\n",
    "\n",
    "Clearly separating out feature columns ensures:\n",
    "\n",
    "Consistent input across different models\n",
    "\n",
    "Flexibility to experiment with subsets of features later\n",
    "\n",
    "Easier debugging and feature importance analysis\n",
    "\n",
    "In uplift modeling, input features are crucial for capturing patterns that help distinguish users likely to respond differently under treatment vs. control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b0d13a-2c52-4e50-a162-2ec817970e2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define columns used for modeling (all renamed features)\n",
    "feature_cols = ['I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12']\n",
    "\n",
    "# Check that all columns exist in the DataFrame\n",
    "missing = [col for col in feature_cols if col not in train_df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected feature columns: {missing}\")\n",
    "\n",
    "print(\"‚úÖ Feature columns defined:\", feature_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa13206-ed34-4bce-a320-572f72581c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5: Split Train Set into Train and Validation Sets\n",
    "## Business Context:\n",
    "\n",
    "We split our original training data into two subsets:\n",
    "\n",
    "Training set (80%): Used to fit our uplift model.\n",
    "\n",
    "Validation set (20%): Used to tune hyperparameters, compare models, and evaluate out-of-sample performance before testing on the final holdout set.\n",
    "\n",
    "This ensures our model does not overfit the training data and that early performance checks are realistic.\n",
    "\n",
    "In a production setting, this would be analogous to simulating future unseen data ‚Äî helping ensure generalization when deployed.\n",
    "\n",
    "## Technical Details:\n",
    "\n",
    "We reuse train_df (from the earlier stratified split) and split it into:\n",
    "\n",
    "train_df: For model training\n",
    "\n",
    "valid_df: For validation\n",
    "\n",
    "We use random_state=42 for reproducibility.\n",
    "\n",
    "We do not stratify here, because the major stratification (by uplift_label) was already done during the initial train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af99777c-6131-40ac-99dd-5b13515ce92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80/20 split for training and validation\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"‚úÖ Train set shape: {train_df.shape}\")\n",
    "print(f\"‚úÖ Validation set shape: {valid_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8863cb24-b006-4df3-9327-b5354517f4f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6: Prepare Labeled Data for Two Uplift Modeling Strategies\n",
    "## Business Context:\n",
    "\n",
    "We now prepare the dataset for uplift modeling, where the business objective is not just to predict conversion, but to predict the incremental effect of treatment (e.g., whether a marketing ad causes a user to convert).\n",
    "\n",
    "To do this, we structure the data to support:\n",
    "\n",
    "Two-Model Approach: Builds separate models for treated and control users, then compares their predicted probabilities to estimate uplift.\n",
    "\n",
    "Class Transformation Approach: Uses a single model but only retains rows where treatment assignment actually impacts interpretation of conversion.\n",
    "\n",
    "This flexibility allows us to benchmark multiple modeling strategies and determine which is best for personalized targeting or treatment allocation.\n",
    "\n",
    "## Technical Details:\n",
    "\n",
    "uplift_binary_label is created where:\n",
    "\n",
    "1 ‚Üí Treated + Converted (treatment success)\n",
    "\n",
    "0 ‚Üí Control + Converted (natural conversion)\n",
    "\n",
    "NaN ‚Üí All other rows (no meaningful comparison)\n",
    "\n",
    "These labels are used only for the Class Transformation method.\n",
    "\n",
    "We also split train_df and valid_df into treated and control subsets for the Two-Model approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a89a1799-d825-44b4-be99-d01fbc98e61f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# üü© Create class transformation label\n",
    "# Only retain rows where class transformation is meaningful\n",
    "train_df['uplift_binary_label'] = np.where(\n",
    "    (train_df['treatment'] == 1) & (train_df['conversion'] == 1), 1,\n",
    "    np.where((train_df['treatment'] == 0) & (train_df['conversion'] == 1), 0, np.nan)\n",
    ")\n",
    "\n",
    "valid_df['uplift_binary_label'] = np.where(\n",
    "    (valid_df['treatment'] == 1) & (valid_df['conversion'] == 1), 1,\n",
    "    np.where((valid_df['treatment'] == 0) & (valid_df['conversion'] == 1), 0, np.nan)\n",
    ")\n",
    "\n",
    "# üü¶ Two-Model Approach: Split treated and control groups\n",
    "train_treated = train_df[train_df['treatment'] == 1]\n",
    "train_control = train_df[train_df['treatment'] == 0]\n",
    "\n",
    "valid_treated = valid_df[valid_df['treatment'] == 1]\n",
    "valid_control = valid_df[valid_df['treatment'] == 0]\n",
    "\n",
    "# üß™ Diagnostic printout\n",
    "print(f\"üì¶ Prepared modeling datasets:\")\n",
    "print(f\"Two-Model: Treated ‚Üí {train_treated.shape} | Control ‚Üí {train_control.shape}\")\n",
    "print(f\"Class Transformation: {train_df[['uplift_binary_label']].dropna().shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ed9f83b-f528-4733-af72-4918db76763b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 7: Train Uplift Models ‚Äì Two Approaches √ó Two Algorithms\n",
    "## Business Context:\n",
    "\n",
    "To determine which users are most likely to respond positively to treatment (e.g., showing ads, offering discounts), we implement two major uplift modeling strategies:\n",
    "\n",
    "Two-Model Approach:\n",
    "\n",
    "Separately trains models on treated and control groups.\n",
    "\n",
    "Uplift is then computed as:\n",
    "Predicted(Conversion|Treated) ‚àí Predicted(Conversion|Control)\n",
    "\n",
    "‚úÖ Intuitive and interpretable, but requires twice the model training and may suffer from bias if groups are imbalanced.\n",
    "\n",
    "Class Transformation Approach:\n",
    "\n",
    "Uses only rows where conversion can be meaningfully attributed to treatment (i.e., Treated + Converted = 1, Control + Converted = 0).\n",
    "\n",
    "Converts uplift into a binary classification task.\n",
    "\n",
    "‚úÖ Simpler, but may lose information due to dropping unrelated rows.\n",
    "\n",
    "Both strategies will be tested with:\n",
    "\n",
    "Logistic Regression (linear, interpretable baseline)\n",
    "\n",
    "Random Forest (nonlinear, high-performing ensemble method)\n",
    "\n",
    "## Technical Summary:\n",
    "Model Type\tStrategy\tTarget Label\tNotes\n",
    "logit_treat\tTwo-Model (Treated)\tconversion\tLogisticRegression on treated users\n",
    "logit_control\tTwo-Model (Control)\tconversion\tLogisticRegression on control users\n",
    "logit_ct\tClass Transformation\tuplift_binary_label\tUses filtered rows with meaningful uplift\n",
    "rf_treat\tTwo-Model (Treated)\tconversion\tRandomForest on treated users\n",
    "rf_control\tTwo-Model (Control)\tconversion\tRandomForest on control users\n",
    "rf_ct\tClass Transformation\tuplift_binary_label\tRandomForest on filtered data\n",
    "\n",
    "Seed is fixed at 42 to ensure reproducibility.\n",
    "\n",
    "All models use feature_cols selected during feature engineering (I1‚ÄìI12).\n",
    "\n",
    "max_iter=1000 ensures logistic regression convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b4463e-2c9d-4b6d-88e8-a5766613f14a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional: Set random_state for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# -------------------------------\n",
    "# üü© Logistic Regression ‚Äì Two Model\n",
    "# -------------------------------\n",
    "logit_treat = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "logit_control = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "\n",
    "logit_treat.fit(train_treated[feature_cols], train_treated['conversion'])\n",
    "logit_control.fit(train_control[feature_cols], train_control['conversion'])\n",
    "\n",
    "# -------------------------------\n",
    "# üü© Logistic Regression ‚Äì Class Transformation\n",
    "# -------------------------------\n",
    "train_ct = train_df.dropna(subset=['uplift_binary_label'])\n",
    "\n",
    "logit_ct = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "logit_ct.fit(train_ct[feature_cols], train_ct['uplift_binary_label'])\n",
    "\n",
    "# -------------------------------\n",
    "# üü¶ Random Forest ‚Äì Two Model\n",
    "# -------------------------------\n",
    "rf_treat = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "rf_control = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "\n",
    "rf_treat.fit(train_treated[feature_cols], train_treated['conversion'])\n",
    "rf_control.fit(train_control[feature_cols], train_control['conversion'])\n",
    "\n",
    "# -------------------------------\n",
    "# üü¶ Random Forest ‚Äì Class Transformation\n",
    "# -------------------------------\n",
    "rf_ct = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "rf_ct.fit(train_ct[feature_cols], train_ct['uplift_binary_label'])\n",
    "\n",
    "print(\"‚úÖ All models trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfbd8fb9-663a-4a71-b466-aba2b2c949da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 8: Predict Uplift Scores ‚Äì All Models on Test Set\n",
    "‚úÖ Purpose:\n",
    "\n",
    "To compare uplift modeling strategies, we predict uplift scores on the same test set using:\n",
    "\n",
    "Two-Model Uplift = Œî(Predicted Conversion) between treated and control models.\n",
    "\n",
    "Class Transformation Uplift = Single model probability for uplift (trained on transformed binary labels).\n",
    "\n",
    "üì¶ Setup:\n",
    "\n",
    "We extract from the test set:\n",
    "\n",
    "X_test: Features used for prediction.\n",
    "\n",
    "T_test: Treatment indicator (used for Qini evaluation later).\n",
    "\n",
    "Y_test: Actual conversion outcomes.\n",
    "\n",
    "üî¢ Predicted Uplift Scores:\n",
    "Model Type\tApproach\tComputation\n",
    "uplift_logit_twomodel\tLogistic Regression, Two-Model\tP(Conv\n",
    "uplift_logit_ct\tLogistic Regression, Class Transformation\tP(Uplift = 1)\n",
    "uplift_rf_twomodel\tRandom Forest, Two-Model\tP(Conv\n",
    "uplift_rf_ct\tRandom Forest, Class Transformation\tP(Uplift = 1)\n",
    "\n",
    "Each prediction gives us a score per user indicating how beneficial it is to assign them to treatment. Higher scores ‚Üí higher expected uplift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6b7ed9-078b-418c-96f7-911e2cff15d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create test set (entire dataset used here for simplicity)\n",
    "X_test = train_df[feature_cols]\n",
    "T_test = train_df['treatment']\n",
    "Y_test = train_df['conversion']\n",
    "\n",
    "# -------------------------------\n",
    "# üü© Logistic Regression ‚Äì Two-Model\n",
    "# -------------------------------\n",
    "p_treat_logit = logit_treat.predict_proba(X_test)[:, 1]\n",
    "p_control_logit = logit_control.predict_proba(X_test)[:, 1]\n",
    "uplift_logit_twomodel = p_treat_logit - p_control_logit\n",
    "\n",
    "# -------------------------------\n",
    "# üü© Logistic Regression ‚Äì Class Transformation\n",
    "# -------------------------------\n",
    "uplift_logit_ct = logit_ct.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# -------------------------------\n",
    "# üü¶ Random Forest ‚Äì Two-Model\n",
    "# -------------------------------\n",
    "p_treat_rf = rf_treat.predict_proba(X_test)[:, 1]\n",
    "p_control_rf = rf_control.predict_proba(X_test)[:, 1]\n",
    "uplift_rf_twomodel = p_treat_rf - p_control_rf\n",
    "\n",
    "# -------------------------------\n",
    "# üü¶ Random Forest ‚Äì Class Transformation\n",
    "# -------------------------------\n",
    "uplift_rf_ct = rf_ct.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"‚úÖ Uplift scores predicted for all models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1361d29-eae5-4ee8-9aba-f38531739bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 9: Visualize Uplift ‚Äì Qini Curves\n",
    "‚úÖ Purpose:\n",
    "\n",
    "Qini curves are a visual tool for evaluating how well uplift models rank individuals in terms of treatment effectiveness.\n",
    "\n",
    "üìä How It Works:\n",
    "\n",
    "The X-axis represents the proportion of the population you target (top-N users based on uplift score).\n",
    "\n",
    "The Y-axis shows incremental conversions due to targeting those users (i.e., uplift).\n",
    "\n",
    "The better the model, the more it separates high-uplift from low-uplift users ‚Üí resulting in a steeper Qini curve.\n",
    "\n",
    "üõ†Ô∏è Models Compared:\n",
    "Label\tModel Description\n",
    "LR (Two-Model)\tLogistic Regression, Two Separate Models\n",
    "LR (Class Trans.)\tLogistic Regression with Transformed Labels\n",
    "RF (Two-Model)\tRandom Forest, Two Separate Models\n",
    "RF (Class Trans.)\tRandom Forest with Transformed Labels\n",
    "\n",
    "‚úÖ Outcome:\n",
    "The plotted curves help determine which model best identifies users who should be targeted for a campaign to maximize uplift. The higher the curve, the better the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68a4302-0cc1-4e12-a31a-51e39eb0eca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install scikit-uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea94198-4b4e-449d-9117-14c5ca544212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklift.metrics import qini_curve\n",
    "from sklift.viz import plot_qini_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare dictionary of model outputs\n",
    "uplift_scores = {\n",
    "    \"LR (Two-Model)\": uplift_logit_twomodel,\n",
    "    \"LR (Class Trans.)\": uplift_logit_ct,\n",
    "    \"RF (Two-Model)\": uplift_rf_twomodel,\n",
    "    \"RF (Class Trans.)\": uplift_rf_ct\n",
    "}\n",
    "\n",
    "# Set plot style\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"üéØ Qini Curves ‚Äì Uplift Model Comparison\", fontsize=14)\n",
    "\n",
    "# Plot each model‚Äôs Qini curve\n",
    "for model_name, uplift in uplift_scores.items():\n",
    "    plot_qini_curve(Y_test.values, uplift, T_test.values, label=model_name)\n",
    "\n",
    "# Final plot touches\n",
    "plt.legend()\n",
    "plt.xlabel(\"Share of Population Targeted (%)\")\n",
    "plt.ylabel(\"Incremental Conversions (Qini)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22c9c83-56a3-4528-bed1-a09005e336d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklift.metrics import qini_auc_score\n",
    "\n",
    "# Store model names and Qini scores\n",
    "qini_scores = {}\n",
    "\n",
    "# Loop through all models and calculate Qini AUC\n",
    "for model_name, uplift in uplift_scores.items():\n",
    "    score = qini_auc_score(y_true=Y_test.values, uplift=uplift, treatment=T_test.values)\n",
    "    qini_scores[model_name] = score\n",
    "\n",
    "# Display scores\n",
    "print(\"üìä Qini Coefficients (Area Under Qini Curve):\")\n",
    "for model, score in sorted(qini_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fe30392-0443-4ef9-acc8-e93741950b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def simulate_gain_curve(y_true, treatment, uplift_scores, model_name):\n",
    "    data = pd.DataFrame({\n",
    "        'treatment': treatment,\n",
    "        'conversion': y_true,\n",
    "        'uplift_score': uplift_scores\n",
    "    })\n",
    "    \n",
    "    data = data.sort_values(by='uplift_score', ascending=False).reset_index(drop=True)\n",
    "    total_samples = len(data)\n",
    "    \n",
    "    # Simulate at 10% intervals\n",
    "    percents = np.arange(0.1, 1.1, 0.1)\n",
    "    incremental_conversions = []\n",
    "    \n",
    "    for pct in percents:\n",
    "        cutoff = int(pct * total_samples)\n",
    "        targeted = data.iloc[:cutoff]\n",
    "        \n",
    "        treated = targeted[targeted['treatment'] == 1]\n",
    "        control = targeted[targeted['treatment'] == 0]\n",
    "        \n",
    "        # Conversion rates\n",
    "        cr_treated = treated['conversion'].mean() if not treated.empty else 0\n",
    "        cr_control = control['conversion'].mean() if not control.empty else 0\n",
    "        uplift = cr_treated - cr_control\n",
    "        \n",
    "        incremental_conversions.append(uplift)\n",
    "    \n",
    "    return percents, incremental_conversions\n",
    "\n",
    "# Plot for each model\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model_name, uplift_scores_model in uplift_scores.items():\n",
    "    x, y = simulate_gain_curve(Y_test.values, T_test.values, uplift_scores_model, model_name)\n",
    "    plt.plot(x * 100, y, label=model_name)\n",
    "\n",
    "plt.title(\"üìà Simulated Incremental Lift vs % Targeted\")\n",
    "plt.xlabel(\"% of Population Targeted (Top Uplift Score)\")\n",
    "plt.ylabel(\"Incremental Conversion Lift\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a88f91f-9a19-47bb-80c4-2940b8e08f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Uplift Curve ‚Äì Explain What the Model is Capturing\n",
    "\n",
    "This curve shows:\n",
    "\n",
    "X-axis: % of population targeted\n",
    "\n",
    "Y-axis: Estimated incremental lift (from model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03021ce0-f39c-4c9b-aaa1-558ed9cff555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklift.metrics import uplift_curve\n",
    "from sklift.viz import plot_uplift_curve\n",
    "\n",
    "# Example with one model\n",
    "plot_uplift_curve(Y_test.values, uplift_logit_ct, T_test.values)\n",
    "plt.title(\"üìà Uplift Curve ‚Äì Logistic Regression (Class Trans.)\")\n",
    "plt.xlabel(\"Share of Population Targeted (%)\")\n",
    "plt.ylabel(\"Estimated Uplift (%)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f3d1b2f-c0f5-4f54-b6c6-f0ee63be0e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Targeting Simulation Chart ‚Äì Where should we stop targeting?\n",
    "\n",
    "This simulation calculates:\n",
    "\n",
    "Uplift per user segment (e.g., deciles)\n",
    "\n",
    "Estimated cost vs. benefit of targeting each segment\n",
    "\n",
    "Helps answer: What % of users should we target to maximize ROI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f512ed2-ea62-4f87-9879-0e70dcb5c8ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df_sim = pd.DataFrame({\n",
    "    'uplift': uplift_logit_ct,\n",
    "    'treatment': T_test.values,\n",
    "    'actual': Y_test.values\n",
    "})\n",
    "\n",
    "# Rank by predicted uplift\n",
    "df_sim['rank'] = df_sim['uplift'].rank(method='first', ascending=False)\n",
    "df_sim = df_sim.sort_values('rank')\n",
    "df_sim['percentile'] = pd.qcut(df_sim['rank'], 10, labels=False)\n",
    "\n",
    "# Compute average conversion uplift per decile\n",
    "uplift_by_decile = df_sim.groupby('percentile').apply(\n",
    "    lambda x: x.loc[x['treatment']==1, 'actual'].mean() - x.loc[x['treatment']==0, 'actual'].mean()\n",
    ").reset_index(name='avg_uplift')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(uplift_by_decile['percentile'], uplift_by_decile['avg_uplift'])\n",
    "plt.xlabel(\"Decile (Predicted Uplift Ranking)\")\n",
    "plt.ylabel(\"Average Incremental Uplift\")\n",
    "plt.title(\"üéØ Uplift by Decile ‚Äì Where Should We Target?\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ef8ea4b-661b-4784-810c-5cedb764512b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Campaign ROI Simulation (Table + Bar Chart)\n",
    "\n",
    "This visualization shows:\n",
    "\n",
    "How many users to target\n",
    "\n",
    "Estimated conversions\n",
    "\n",
    "Cost per treatment\n",
    "\n",
    "Revenue per conversion\n",
    "\n",
    "Net ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e4bf9a-24eb-4ad9-8ee0-9b786adf0447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cost_per_user = 2     # marketing cost\n",
    "revenue_per_conversion = 50\n",
    "\n",
    "results = []\n",
    "percentiles = range(1, 11)\n",
    "\n",
    "for p in percentiles:\n",
    "    cutoff = int(len(df_sim) * p / 10)\n",
    "    top_users = df_sim.sort_values('uplift', ascending=False).head(cutoff)\n",
    "    \n",
    "    treated = top_users[top_users['treatment'] == 1]\n",
    "    control = top_users[top_users['treatment'] == 0]\n",
    "    \n",
    "    uplift = treated['actual'].mean() - control['actual'].mean()\n",
    "    n_users = len(top_users)\n",
    "    est_conversions = uplift * n_users\n",
    "    revenue = est_conversions * revenue_per_conversion\n",
    "    cost = n_users * cost_per_user\n",
    "    roi = revenue - cost\n",
    "    \n",
    "    results.append({\n",
    "        'Percentile Targeted': p*10,\n",
    "        'Users Targeted': n_users,\n",
    "        'Estimated Uplifted Conversions': est_conversions,\n",
    "        'Revenue': revenue,\n",
    "        'Cost': cost,\n",
    "        'ROI': roi\n",
    "    })\n",
    "\n",
    "df_roi = pd.DataFrame(results)\n",
    "\n",
    "# Plot ROI vs. Percentile Targeted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_roi['Percentile Targeted'], df_roi['ROI'], marker='o')\n",
    "plt.xlabel(\"% of Users Targeted\")\n",
    "plt.ylabel(\"Net ROI\")\n",
    "plt.title(\"üìä Campaign ROI Simulation by Targeting Range\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d385171e-fd3d-4d37-9240-e39421df65fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8224ec17-3b4f-4af8-a996-6f2e2b3ff04a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28d36ce6-e35b-4e3d-8210-25657a420a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "283b42da-2df7-48bb-a55a-0705c2f7bf2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_uplift_model_visualization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
