{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1115c95-8e36-4c65-9743-24a49cad9dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Set the storage account name\n",
    "storage_account = \"stcampaigntp\"  # update if your storage account is different\n",
    "\n",
    "# Step 2: Mount access using secret\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "  dbutils.secrets.get(scope=\"local-scope\", key=\"storage-account-key\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dbd37f0-bc91-4165-acc4-abf5b094b656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### STEP 1:Load and Feature Engineer Full Dataset \n",
    "\n",
    "## Business Context:\n",
    "\n",
    "To enable scalable and efficient uplift modeling, we start by preparing a clean and structured version of the raw Criteo ad dataset. The original data includes 12 continuous features (f0–f11) along with treatment and outcome variables. However, to streamline feature handling and align with naming conventions used in marketing and experimentation analytics, we rename these features (I1–I12), enforce consistent data types, and remove any corrupted or incomplete records.\n",
    "\n",
    "This step ensures the data is reliable, interpretable, and ready for downstream modeling and analysis pipelines that support leadership decision-making.\n",
    "\n",
    "## Technical Details:\n",
    "\n",
    "Load Data: We read in the curated 1M-row sample (previously stored in the curated Azure Data Lake container) using PySpark for distributed processing.\n",
    "\n",
    "Rename Features: Columns f0 to f11 are renamed to I1 to I12 to better represent interpretable feature identifiers.\n",
    "\n",
    "Data Cleaning:\n",
    "\n",
    "We remove rows with any missing values using .dropna().\n",
    "\n",
    "We enforce a consistent IntegerType for all columns, including features, treatment indicator, and outcome.\n",
    "\n",
    "Save Cleaned Dataset: The cleaned and transformed dataset is saved back to the curated container under a new path: criteo-1m-features.\n",
    "\n",
    "This processing ensures the dataset is clean, typed, and optimized for Spark-based transformations and distributed model training workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "277ff537-9967-4438-9a77-dbc6d8ad1abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5A: Load sampled data\n",
    "df = spark.read.parquet(f\"abfss://curated@{storage_account}.dfs.core.windows.net/criteo_uplift_1m\")\n",
    "print(f\"✅ Loaded {df.count():,} rows\")\n",
    "\n",
    "# 5B: Feature engineering\n",
    "\n",
    "# Optional: Rename f0–f11 to meaningful names like I1–I12\n",
    "new_col_names = {\n",
    "    'f0': 'I1', 'f1': 'I2', 'f2': 'I3', 'f3': 'I4',\n",
    "    'f4': 'I5', 'f5': 'I6', 'f6': 'I7', 'f7': 'I8',\n",
    "    'f8': 'I9', 'f9': 'I10', 'f10': 'I11', 'f11': 'I12'\n",
    "}\n",
    "\n",
    "for old_name, new_name in new_col_names.items():\n",
    "    df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "# Drop any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Cast all I-columns and labels to IntegerType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "for col in df.columns:\n",
    "    df = df.withColumn(col, df[col].cast(IntegerType()))\n",
    "\n",
    "# Show schema to confirm\n",
    "df.printSchema()\n",
    "\n",
    "# Save processed features to 'curated' container\n",
    "df.write.mode(\"overwrite\").parquet(f\"abfss://curated@{storage_account}.dfs.core.windows.net/criteo-1m-features\")\n",
    "print(\"✅ Feature-engineered data written to curated/criteo-1m-features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33d7cf2e-3794-48c6-82bf-aca8f0dfcf15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Why Are We Performing Sampling?\n",
    "\n",
    "This dataset contains 1 million observations from a real-world ad campaign with a binary treatment variable:\n",
    "- `treatment = 1`: user was shown the ad\n",
    "- `treatment = 0`: user was not shown the ad (control group)\n",
    "\n",
    "However, the treatment distribution is highly **imbalanced**:\n",
    "- ~85% of the users were treated\n",
    "- ~15% were in control\n",
    "\n",
    "#### Business Implication:\n",
    "In uplift modeling, the model needs to learn **differential responses** between the treatment and control groups. If one group (control) is underrepresented, the model will struggle to accurately estimate the **causal effect** of the treatment (e.g., whether the ad really changed behavior).\n",
    "\n",
    "#### Technical Implication:\n",
    "We apply **stratified sampling** to:\n",
    "1. **Reduce overall data size** for fast iteration and modeling (100k rows instead of 1M)\n",
    "2. **Balance the treatment and control groups** to avoid bias in uplift estimation\n",
    "\n",
    "This sampling improves both performance and reliability, making our model better at identifying which users to target in future campaigns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daceb51a-c98d-4fc5-9027-84bb47da7915",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## You might ask questions like:\n",
    "\n",
    "“If the original dataset has 85% treated and 15% control, and we use stratified sampling — aren’t we just keeping the same imbalance in the sampled dataset? What’s the point of sampling then?”\n",
    "\n",
    "# Short answer to this is:\n",
    "\n",
    "You're right that the class imbalance remains the same after stratified sampling — because we intentionally preserved it. However, the goal of sampling here is not to fix the imbalance, but to reduce dataset size while maintaining the same structure for modeling.\n",
    "\n",
    "# Long answer with business + technical perspective:\n",
    "Business Logic:\n",
    "\n",
    "In real-world marketing or product campaigns (like Criteo’s), most users do get exposed to the treatment (ad), while a smaller set remains in control.\n",
    "\n",
    "If we try to \"balance\" this artificially during modeling, the uplift model wouldn’t reflect real campaign distribution.\n",
    "\n",
    "So we want the model to learn patterns based on realistic exposure rates.\n",
    "\n",
    "Technical Explanation:\n",
    "\n",
    "We use stratified sampling to keep the proportion of treated/control the same as the full dataset.\n",
    "\n",
    "If we didn't do stratified sampling, we might accidentally select too few control users — which would break uplift modeling, as it relies on comparing treated vs control groups.\n",
    "\n",
    "So:\n",
    "\n",
    "Imbalance is kept intentionally.\n",
    "\n",
    "Sampling only reduces size (for faster iteration and less memory use), not for balancing.\n",
    "\n",
    "Important Note:\n",
    "\n",
    "If we did want to test a balanced setting (say 50% treated / 50% control) for research purposes, we would do deliberate under-sampling of treated users — but that would be a separate experiment, not a default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4126eb7f-8e04-4c51-977d-aca4923bbe07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Stratified Sampling — 15% from each treatment group\n",
    "fractions = {0: 0.15, 1: 0.15}\n",
    "sampled_df = df.sampleBy(\"treatment\", fractions=fractions, seed=42).cache()\n",
    "\n",
    "# Step 2: Show count by treatment group\n",
    "sampled_df.groupBy(\"treatment\").count().show()\n",
    "\n",
    "# Step 3: Select only needed columns\n",
    "selected_cols = [\"treatment\", \"conversion\"] + [c for c in df.columns if c.startswith(\"feature_\")]\n",
    "sampled_df = sampled_df.select([col(c) for c in selected_cols])\n",
    "\n",
    "# Step 4: Optional — Create temp SQL view for inspection\n",
    "sampled_df.createOrReplaceTempView(\"sampled_criteo\")\n",
    "\n",
    "# Step 5: Convert to Pandas\n",
    "pandas_df = sampled_df.toPandas()\n",
    "\n",
    "# Step 6: Plot with value labels\n",
    "ax = pandas_df['treatment'].value_counts(normalize=True).sort_index().plot(\n",
    "    kind='bar', \n",
    "    title='Treatment Group Distribution (Sampled)',\n",
    "    color=['orange', 'skyblue']\n",
    ")\n",
    "ax.set_xlabel(\"Treatment Group\")\n",
    "ax.set_ylabel(\"Proportion\")\n",
    "ax.set_xticklabels(['Control (0)', 'Treatment (1)'], rotation=0)\n",
    "\n",
    "# Add % labels on bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height()*100:.1f}%\", \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48f73e4b-7f3e-4e11-8b23-7e702f721f62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Label engineering\n",
    "\n",
    "## Business Context:\n",
    "To train an uplift model, we can’t just use a traditional \"conversion\" target. We need to define how the treatment affected the conversion, i.e., whether the treatment caused the outcome.\n",
    "\n",
    "This is done using the 4-way uplift classification:\n",
    "\n",
    "Class 0: Not Treated → Did Not Convert\n",
    "\n",
    "Class 1: Not Treated → Converted\n",
    "\n",
    "Class 2: Treated → Did Not Convert\n",
    "\n",
    "Class 3: Treated → Converted\n",
    "\n",
    "These four classes help the model distinguish between natural converters vs treatment-driven converters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b9f156a-99c9-48ad-9175-e75b2c58575e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assuming your DataFrame has 'treatment' and 'conversion' columns\n",
    "# Encode uplift label: 2*treatment + conversion → gives 0 to 3\n",
    "pandas_df[\"uplift_label\"] = 2 * pandas_df[\"treatment\"] + pandas_df[\"conversion\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cd60ae6-fef0-46db-a387-22d213b5eda5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Define uplift label meanings\n",
    "uplift_label_names = {\n",
    "    0: 'Control - No Conversion',\n",
    "    1: 'Control - Conversion',\n",
    "    2: 'Treated - No Conversion',\n",
    "    3: 'Treated - Conversion'\n",
    "}\n",
    "\n",
    "# Step 2: Count and relabel uplift label distribution\n",
    "uplift_counts = pandas_df['uplift_label'].value_counts().sort_index()\n",
    "uplift_counts.index = uplift_counts.index.map(uplift_label_names)\n",
    "\n",
    "# Step 3: Plot uplift label distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(uplift_counts.index, uplift_counts.values, color='steelblue')\n",
    "plt.title(\"Uplift Label Distribution\")\n",
    "plt.ylabel(\"Number of Users\")\n",
    "plt.xlabel(\"Uplift Label Category\")\n",
    "\n",
    "# Step 4: Annotate values on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.annotate(f'{height:,}', \n",
    "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                 xytext=(0, 3), \n",
    "                 textcoords=\"offset points\",\n",
    "                 ha='center', va='bottom')\n",
    "\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfbd47b0-f35a-47cd-b982-852a362b145b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the processed pandas dataframe for modeling\n",
    "pandas_df.to_parquet(\"pandas_uplift_sampled.parquet\", index=False)\n",
    "print(\"✅ Saved processed dataframe to pandas_uplift_sampled.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_eda_features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
